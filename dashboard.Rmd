---
title: "Project #3: Escapades in Market Risk"
author:
- Nicolas Schoonmaker
- Guillermo Delgado
- Katie Guillen
- Leanne Harper
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: flexdashboard::flex_dashboard
subtitle: 'Lastname-Firstname_Project3'
---

```{r, include=FALSE}
# ----------------------- Setup section -------------------------
```

```{r Packages and Installers, echo=FALSE}
# Install RTools
# https://cran.rstudio.com/bin/windows/Rtools/
#
# Restart R Studio
#
# Install packages
#install.packages("dplyr")
#install.packages("rstudioapi")
#install.packages("tinytex")
#install.packages("magick")
#install.packages("plotly")
#install.packages("xts")
#install.packages("ggplot2")
#install.packages("moments")
#install.packages("matrixStats")
#install.packages("quantreg")
#install.packages("flexdashboard")
#install.packages("QRM")
#install.packages("qrmdata")
```

```{r Library Includes, echo=FALSE}
# The list of libraries to include
library(stats)
library(dplyr)
library(rstudioapi)
library(flexdashboard)
```

```{r Get current directory, echo=FALSE}
# Get the directory so we can run this from anywhere
# Get the script directory from R when running in R
if(rstudioapi::isAvailable())
{
  #print("Running in RStudio")
  script.path <- rstudioapi::getActiveDocumentContext()$path
  #(script.path)
  script.dir <- dirname(script.path)
}
if(!exists("script.dir"))
{
  #print("Running from command line")
  script.dir <- getSrcDirectory(function(x) {x})
}
#(script.dir)
```

```{r Working Directory and Data setup, echo=FALSE}
# Set my working directory
# There is a "data" folder here with the files and the script
setwd(script.dir)
# Double check the working directory
#getwd()
# Error check to ensure the working directory is set up and the data
# directory exists inside it.  Its required for this file
working_dir <- getwd()
full_path <- paste(working_dir,"/data", sep = "")
data_dir_exists <- dir.exists(full_path)
if(data_dir_exists == FALSE) {
  stop("Data directory does not exist. Make sure the working directory
       is set using setwd() and the data folder exists in it.")
}# else {
  #print("Working directory and data set up correctly")
#}
```

```{r, include=FALSE}
# ----------------------- Project code section -------------------------
```

```{r, echo=FALSE }
library(ggplot2)
library(flexdashboard)
library(shiny)
library(QRM)
library(qrmdata)
library(xts)
library(zoo)
library(psych)

rm(list = ls())

# PAGE: Exploratory Analysis
data <- na.omit(read.csv("data/metaldata.csv", header = TRUE))
prices <- data
# Compute log differences percent using as.matrix to force numeric type
data.r <- diff(log(as.matrix(data[, -1]))) * 100
# Create size and direction
size <- na.omit(abs(data.r)) # size is indicator of volatility
#head(size)
colnames(size) <- paste(colnames(size),".size", sep = "") # Teetor
direction <- ifelse(data.r > 0, 1, ifelse(data.r < 0, -1, 0)) # another indicator of volatility
colnames(direction) <- paste(colnames(direction),".dir", sep = "")
# Convert into a time series object: 
# 1. Split into date and rates
dates <- as.Date(data$DATE[-1], "%m/%d/%Y")
dates.chr <- as.character(data$DATE[-1])
#str(dates.chr)
values <- cbind(data.r, size, direction)
# for dplyr pivoting and ggplot2 need a data frame also known as "tidy data"
data.df <- data.frame(dates = dates, returns = data.r, size = size, direction = direction)
data.df.nd <- data.frame(dates = dates.chr, returns = data.r, size = size, direction = direction, stringsAsFactors = FALSE) 
#non-coerced dates for subsetting on non-date columns
# 2. Make an xts object with row names equal to the dates
data.xts <- na.omit(as.xts(values, dates)) #order.by=as.Date(dates, "%d/%m/%Y")))
#str(data.xts)
data.zr <- as.zooreg(data.xts)
returns <- data.xts # watch for this data below!

# PAGE: Market risk 
corr_rolling <- function(x) {	
  dim <- ncol(x)	
  corr_r <- cor(x)[lower.tri(diag(dim), diag = FALSE)]	
  return(corr_r)	
}
vol_rolling <- function(x){
  library(matrixStats)
  vol_r <- colSds(x)
  return(vol_r)
}
ALL.r <- data.xts[, 1:3]
window <- 90 #reactive({input$window})
corr_r <- rollapply(ALL.r, width = window, corr_rolling, align = "right", by.column = FALSE)
colnames(corr_r) <- c("nickel.copper", "nickel.aluminium", "copper.aluminium")
vol_r <- rollapply(ALL.r, width = window, vol_rolling, align = "right", by.column = FALSE)
colnames(vol_r) <- c("nickel.vol", "copper.vol", "aluminium.vol")
year <- format(index(corr_r), "%Y")
r_corr_vol <- merge(ALL.r, corr_r, vol_r, year)
# Market dependencies
#library(matrixStats)
R.corr <- apply.monthly(as.xts(ALL.r), FUN = cor)
R.vols <- apply.monthly(ALL.r, FUN = colSds) # from MatrixStats	
# Form correlation matrix for one month 	
R.corr.1 <- matrix(R.corr[20,], nrow = 3, ncol = 3, byrow = FALSE)	
rownames(R.corr.1) <- colnames(ALL.r[,1:3])	
colnames(R.corr.1) <- rownames(R.corr.1)	
#R.corr.1
R.corr <- R.corr[, c(2, 3, 6)]
colnames(R.corr) <- c("nickel.copper", "nickel.aluminium", "copper.aluminium") 	
colnames(R.vols) <- c("nickel.vols", "copper.vols", "aluminium.vols")	
R.corr.vols <- na.omit(merge(R.corr, R.vols))
R.corr.vols.logs <- na.omit(log(R.corr.vols))
year <- format(index(R.corr.vols), "%Y")
R.corr.vols.y <- data.frame(nickel.correlation = R.corr.vols[,1], copper.volatility = R.corr.vols[,5], year = year)
nickel.vols <- as.numeric(R.corr.vols[,"nickel.vols"])	
copper.vols <- as.numeric(R.corr.vols[,"copper.vols"])	
aluminium.vols <- as.numeric(R.corr.vols[,"aluminium.vols"])
```

```{r, include=FALSE}
# ----------------------- Background section -------------------------
```

Background {data-orientation=columns}
=====================================  
    
Column {data-width=650}
-------------------------------------
    
### Problem

A freight forwarder with a fleet of bulk carriers wants to optimize their portfolio in  metals markets with entry into the nickel business and use of the tramp trade.  Tramp ships are the company's "swing" option without any fixed charter or other constraint. They allow the company flexibility in managing several aspects of freight uncertainty. The call for tramp transportation is a "derived demand" based on the value of the cargoes. This value varies widely in the spot markets. The company allocates \$250 million to manage receivables. The company wants us to:

1.	Retrieve and begin to analyze data about potential commodities for diversification,
2.	Compare potential commodities with existing commodities in conventional metal spot markets,
3.	Begin to generate economic scenarios based on events that may, or may not, materialize in the commodities.
4.	The company wants to mitigate their risk by diversifying their cargo loads. This risk measures the amount of capital the company needs to maintain its portfolio of services.

### Additional details

1.	Product: Metals commodities and freight charters
2.	Metal, Company, and Geography:
    a. Nickel: MMC Norilisk, Russia
    b. Copper: Codelco, Chile and MMC Norilisk, Russia
    c. Aluminium: Vale, Brasil and Rio Tinto Alcan, Australia
3.	Customers: Ship Owners, manufacturers, traders
4.  All metals are traded on the London Metal Exchange  

Using RMD from: https://wgfoote.github.io/fin-alytics/HTML/PR03_market-risk.html  
Our code is on github at: https://github.com/nschoonm/FIN654-Project3

### Key business questions

1.	How would the performance of these commodities affect the size and timing of shipping arrangements?
2.	How would the value of new shipping arrangements affect the value of our business with our current customers?
3.	How would we manage the allocation of existing resources given we have just landed in this new market?

Column {data-width=350}
-------------------------------------
   
### Getting a response: More detailed questions

These detailed questions are answered in part by the tables, graphs and models developed - add commentary as needed to explain the outputs

1. What is the decision the freight-forwarder must make? List key business questions and data needed to help answer these questions and support the freight-forwarder's decision. Retrieve data and build financial market detail into the data story behind the questions.

2. Develop the stylized facts of the markets the freight-forwarder faces. Include level, returns, size times series plots. Calculate and display in a table the summary statistics, including quantiles, of each of these series. Use autocorrelation, partial autocorrelation, and cross correlation functions to understand some of the persistence of returns including leverage and volatility clustering effects. Use quantile regressions to develop the distribution of sensitivity of each market to spill-over effects from other markets. Interpret these stylized "facts" in terms of the business decision the freight-forwarder makes.

3. How much capital would the freight-forwarder need? Determine various measures of risk in the tail of each metal's distribution. Then figure out a loss function to develop the portfolio of risk, and the determination of risk capital the freight-forwarder might need. Confidence intervals might be used to create a risk management plan with varying tail experience thresholds.

```{r include=FALSE}
# --------------------- Data Exploration section -----------------------
```

Data Exploration {data-orientation=rows}
=====================================     
   
Row {data-height=600}
-------------------------------------

### Metals Market Percent Changes

```{r}
title.chg <- "Metals Market Percent Changes"
autoplot.zoo(data.xts[,1:3]) + ggtitle(title.chg) + ylim(-5, 5)
```

### Metals Market Percent Changes - Size

```{r}
title.chg <- "Metals Market Percent Changes - Size"
autoplot.zoo(data.xts[,4:6]) + ggtitle(title.chg) + ylim(-5, 5)
```

### Data
Descriptive Statistics
```{r}
# Add min/1st qu/median/srd qu/max of nickel/copper/alum
summary(data.r, digits=3)
```

```{r}
# Add min/1st qu/median/srd qu/max of nickel/copper/alum
summary(size, digits=3)
```
<hr>
Data Moments
```{r}
# Output the copper.size, alum.size, nickel.dir and copper.dir
# this should be mean, median, std_dev, IQR, skewness, kurtosis
#
# Load the data_moments() function
# data_moments function
# INPUTS: r vector
# OUTPUTS: list of scalars (mean, sd, median, skewness, kurtosis)
data_moments <- function(data){
  library(moments)
  library(matrixStats)
  mean.r <- colMeans(data)
  median.r <- colMedians(data)
  sd.r <- colSds(data)
  IQR.r <- colIQRs(data)
  skewness.r <- skewness(data)
  kurtosis.r <- kurtosis(data)
  result <- data.frame(mean = mean.r, median = median.r, std_dev = sd.r, IQR = IQR.r, skewness = skewness.r, kurtosis = kurtosis.r)
  return(result)
}
# Run data_moments()
answer <- data_moments(data.xts[, 4:6])
# Build pretty table
answer <- round(answer, 3)
knitr::kable(answer)

answer <- data_moments(data.xts[, 1:3])
# Build pretty table
answer <- round(answer, 3)
knitr::kable(answer)
```

Row {data-height=400}
-------------------------------------
    
### Data Insights

[Add data insights from graphs and data]
<br/><br/>

[Talk abut time frame, volatility, correlation, skewness andkurtosis]


```{r include=FALSE}
# --------------------- Volatility section -----------------------
```

Volatility {data-orientation=rows}
=====================================     
   
Row {data-height=300}
-------------------------------------

### Returns % change - ACF

Returns % change - ACF
```{r}
# Add graphs for Returns % change - ACF
```

### Size Returns % change - ACF

Size Returns % change - ACF
```{r}
# Add graphs for Size Returns % change - ACF
```

Row {data-height=300}
-------------------------------------
    
### Returns % change - PACF

Returns % change - ACF
```{r}
# Add graphs for Returns % change - PACF
```

### Size Returns % change - PACF

Size Returns % change - ACF
```{r}
# Add graphs for Size Returns % change - PACF
```

Row {data-height=300}
-------------------------------------

### Data Insights

[Add data insights. Something about weekly vs monthly, tails, volatility, etc ]


```{r include=FALSE}
# --------------------- Correlation Sensitivity section -----------------------
```

Correlation Sensitivity {data-orientation=rows}
=====================================     
   
Row {data-height=700}
-------------------------------------

### nickel-copper correlation sensitivity to copper volatility

nickel-copper correlation sensitivity to copper volatility
```{r}
# Add graphs for nickel-copper correlation sensitivity to copper volatility
```

### copper-aluminum correlation sensitivity to aluminum volatility

copper-aluminum correlation sensitivity to aluminum volatility
```{r}
# Add graphs for copper-aluminum correlation sensitivity to aluminum volatility
```

### nickel-aluminum correlation sensitivity to aluminum volatility

nickel-aluminum correlation sensitivity to aluminum volatility
```{r}
# Add graphs for nickel-aluminum correlation sensitivity to aluminum volatility
```

Row {data-height=300}
-------------------------------------

### Data Insights

[Add data insights. Talk about variance, spikes, positive vs negative]

```{r include=FALSE}
# --------------------- Expected Shortfall section -----------------------
```

Expected Shortfall {data-orientation=rows}
=====================================     
   
Row {data-height=700}
-------------------------------------

### Nickel Shortfall

Nickel Shortfall
```{r}
# Add graphs for Nickel Shortfall
```

### Copper Shortfall

Copper Shortfall
```{r}
# Add graphs for Copper Shortfall
```

### Aluminum Shortfall

Aluminum Shortfall
```{r}
# Add graphs for Aluminum Shortfall
```

Row {data-height=300}
-------------------------------------

### Data Insights

[Add data insights. Which has highest risk, tails, expected shortfall values and value at risk values, what do they mean?]

```{r include=FALSE}
# --------------------- Losses section -----------------------
```

Losses {data-orientation=rows}
=====================================     
   
Row {data-height=700}
-------------------------------------

### Losses Graph

Losses Graph
```{r}
# Add graphs for Losses
```

Row {data-height=300}
-------------------------------------

### Data Insights

[Add data insights. What is the loss limit, VAR, ES, what do they mean?  What about the distribution?]

```{r include=FALSE}
# --------------------- Extremes section -----------------------
```

Extremes {data-orientation=rows}
=====================================     
   
Row {data-height=700}
-------------------------------------

### Estimated tail probabilities for confidence interval x

Estimated tail probabilities
```{r}
# Add graphs for Estimated tail probabilities for confidence interval x
```

### Estimated tail probabilities for confidence interval y

Estimated tail probabilities
```{r}
# Add graphs for Estimated tail probabilities for confidence interval y
```

Row {data-height=300}
-------------------------------------

### Data Insights

[Add data insights. Little about GPD, high thresholds vs low, what happens?  What is it more accurate?]

```{r include=FALSE}
# --------------------- Conclusion section -----------------------
```

Conclusion {data-orientation=rows}
=====================================     
   
Row {data-height=400}
-------------------------------------

### Skills and Tools

[List Skills and Tools]

### Data Insights

[Data Insights details]

Row {data-height=600}
-------------------------------------

### Business Remarks

[Add business remarks]